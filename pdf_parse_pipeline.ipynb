{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import fitz  # PyMuPDF\n",
    "import concurrent.futures\n",
    "from pymongo import MongoClient\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Parsing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf parsing function\n",
    "\n",
    "logging.basicConfig(filename='pdf_pipeline.log', level=logging.ERROR)\n",
    "\n",
    "def par_pdf(file_path):\n",
    "    try:\n",
    "        doc = fitz.open(file_path)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "        doc.close()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error processing {file_path} : \" + str(e))\n",
    "        return None\n",
    "    \n",
    "# import pdfs from a folder\n",
    "def import_pdfs(folder_path):\n",
    "    pdf_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(par_pdf, pdf_files)\n",
    "    \n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Store Metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mongoDB setup on localhost\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client['pdf_summarization']\n",
    "collection = db['pdf_documents']\n",
    "\n",
    "# function to store metadata\n",
    "def store(file_path, text):\n",
    "    try:\n",
    "        metadata = {\n",
    "            \"file_name\": os.path.basename(file_path),\n",
    "            \"file_path\": file_path,\n",
    "            \"size\": os.path.getsize(file_path),\n",
    "            \"summary\": None,\n",
    "            \"keywords\": None\n",
    "        }\n",
    "        collection.insert_one(metadata)\n",
    "        print(\"metadata stored for\" + str({metadata ['file_name']}))\n",
    "    except Exception as e:\n",
    "        print(\" error storing metadata\" + str(e))\n",
    "\n",
    "# funtion to update summary and keywords function\n",
    "def update(file_name,summary, keywords):\n",
    "    try:\n",
    "        collection.update_one(\n",
    "            {\"file_name\": file_name},\n",
    "            {\"$set\": {\"summary\":summary ,\"keywords\": keywords}}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"error updating metadata: \" + str(e))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import PyPDF2\n",
    "from collections import defaultdict\n",
    "\n",
    "def summarize_text(text, num_sentences=3):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create a dictionary to hold sentence scores\n",
    "    sentence_scores = defaultdict(int)\n",
    "\n",
    "    # Score each sentence based on noun and adjective frequencies\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\"] and not token.is_stop:\n",
    "                sentence_scores[sent.text] += 1  # Score sentences based on important words\n",
    "\n",
    "    # Select the top N sentences as the summary\n",
    "    summarized_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
    "    \n",
    "    return ' '.join(summarized_sentences)\n",
    "\n",
    "# Loading the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_keywords(text):\n",
    "    doc = nlp(text.lower())\n",
    "    keywords = [token.text for token in doc if token.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\"] and not token.is_stop]\n",
    "    return [keyword for keyword, _ in Counter(keywords).most_common(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata stored for{'Biological_pretreatment_of_lignocellulosic_biomass_An_environment.pdf'}\n",
      "metadata stored for{'Persons with partial work ability at work - PDF Room.pdf'}\n",
      "metadata stored for{'Joy at Work Work at Joy_ Living and Working Mindfully Every Day - PDF Room.pdf'}\n",
      "metadata stored for{'Donâ€™t Sweat the Small Stuff at Work - PDF Room.pdf'}\n"
     ]
    }
   ],
   "source": [
    "def process_pdf(file_path):\n",
    "    text = par_pdf(file_path)\n",
    "    if text:\n",
    "        summary = summarize_text(text)\n",
    "        keywords = extract_keywords(text)\n",
    "        # Store \n",
    "        store(file_path, text)\n",
    "        # Update \n",
    "        update(os.path.basename(file_path),summary, keywords)\n",
    "\n",
    "def main(folder_path):\n",
    "    pdf_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        executor.map(process_pdf, pdf_files)\n",
    "\n",
    "# Example run\n",
    "main(\"pdfs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
